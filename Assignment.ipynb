{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4e01938-a7f7-4758-852a-7088e829cfe7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ingest members, diagnosis, claims csv into bronze"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <html>\n",
       "      <div style=\"font-size:18px\">\n",
       "        The Delta Live Tables (DLT) module is not supported on this cluster.\n",
       "        You should either <a href=\"?o=1142538941258186#joblist/pipelines/create?initialSource=%2FUsers%2Fpratchayapol%40gmail.com%2FCapstone+Project&redirectNotebookId=2559331499754930\">create a new pipeline</a> or use an existing pipeline to run DLT code.\n",
       "      </div>\n",
       "    </html>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)\n",
       "File \u001b[0;32m<command-4760633582222195>, line 1\u001b[0m\n",
       "\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdlt\u001b[39;00m\n",
       "\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m col\n",
       "\u001b[1;32m      4\u001b[0m \u001b[38;5;129m@dlt\u001b[39m\u001b[38;5;241m.\u001b[39mtable(\n",
       "\u001b[1;32m      5\u001b[0m   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcapstone_hospital.bronze_members\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
       "\u001b[1;32m      6\u001b[0m   comment\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaw members data ingested to Bronze layer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
       "\u001b[0;32m   (...)\u001b[0m\n",
       "\u001b[1;32m     10\u001b[0m )\n",
       "\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbronze_members\u001b[39m():\n",
       "\n",
       "File \u001b[0;32m/databricks/python_shell/lib/dbruntime/autoreload/discoverability/autoreload_discoverability_hook.py:96\u001b[0m, in \u001b[0;36mAutoreloadDiscoverabilityHook._patched_import\u001b[0;34m(self, name, *args, **kwargs)\u001b[0m\n",
       "\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_hint \u001b[38;5;129;01mand\u001b[39;00m (\n",
       "\u001b[1;32m     91\u001b[0m     (module \u001b[38;5;241m:=\u001b[39m sys\u001b[38;5;241m.\u001b[39mmodules\u001b[38;5;241m.\u001b[39mget(absolute_name)) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n",
       "\u001b[1;32m     92\u001b[0m     (fname \u001b[38;5;241m:=\u001b[39m get_allowed_file_name_or_none(module)) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n",
       "\u001b[1;32m     93\u001b[0m     (mtime \u001b[38;5;241m:=\u001b[39m os\u001b[38;5;241m.\u001b[39mstat(fname)\u001b[38;5;241m.\u001b[39mst_mtime) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_mtime_by_modname\u001b[38;5;241m.\u001b[39mget(\n",
       "\u001b[1;32m     94\u001b[0m         absolute_name, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_hint):\n",
       "\u001b[1;32m     95\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_hint \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
       "\u001b[0;32m---> 96\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_builtins_import(name, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
       "\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (fname \u001b[38;5;241m:=\u001b[39m fname \u001b[38;5;129;01mor\u001b[39;00m get_allowed_file_name_or_none(module)) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
       "\u001b[1;32m     98\u001b[0m     mtime \u001b[38;5;241m=\u001b[39m mtime \u001b[38;5;129;01mor\u001b[39;00m os\u001b[38;5;241m.\u001b[39mstat(fname)\u001b[38;5;241m.\u001b[39mst_mtime\n",
       "\n",
       "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dlt'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "ModuleNotFoundError",
        "evalue": "No module named 'dlt'"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>ModuleNotFoundError</span>: No module named 'dlt'"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
        "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
        "File \u001b[0;32m<command-4760633582222195>, line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdlt\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m col\n\u001b[1;32m      4\u001b[0m \u001b[38;5;129m@dlt\u001b[39m\u001b[38;5;241m.\u001b[39mtable(\n\u001b[1;32m      5\u001b[0m   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcapstone_hospital.bronze_members\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m   comment\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaw members data ingested to Bronze layer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbronze_members\u001b[39m():\n",
        "File \u001b[0;32m/databricks/python_shell/lib/dbruntime/autoreload/discoverability/autoreload_discoverability_hook.py:96\u001b[0m, in \u001b[0;36mAutoreloadDiscoverabilityHook._patched_import\u001b[0;34m(self, name, *args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_hint \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m     91\u001b[0m     (module \u001b[38;5;241m:=\u001b[39m sys\u001b[38;5;241m.\u001b[39mmodules\u001b[38;5;241m.\u001b[39mget(absolute_name)) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     (fname \u001b[38;5;241m:=\u001b[39m get_allowed_file_name_or_none(module)) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m     93\u001b[0m     (mtime \u001b[38;5;241m:=\u001b[39m os\u001b[38;5;241m.\u001b[39mstat(fname)\u001b[38;5;241m.\u001b[39mst_mtime) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_mtime_by_modname\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m     94\u001b[0m         absolute_name, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_hint):\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_hint \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_builtins_import(name, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (fname \u001b[38;5;241m:=\u001b[39m fname \u001b[38;5;129;01mor\u001b[39;00m get_allowed_file_name_or_none(module)) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     98\u001b[0m     mtime \u001b[38;5;241m=\u001b[39m mtime \u001b[38;5;129;01mor\u001b[39;00m os\u001b[38;5;241m.\u001b[39mstat(fname)\u001b[38;5;241m.\u001b[39mst_mtime\n",
        "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dlt'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "#Bronze Load for MemberData CSV\n",
    "@dlt.table(\n",
    "    comment=\"Bronze Load for MemberData CSV\",\n",
    "    table_properties={\n",
    "        \"quality\": \"bronze\",\n",
    "        \"pipelines.autoOptimize.managed\": \"true\"\n",
    "    }\n",
    ")\n",
    "def BronzeMembers():\n",
    "    df = spark.read.format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .load(\"/Volumes/capstone_hospital/data/raw/members.csv\")\n",
    "    return df\n",
    "\n",
    "\n",
    "#Bronze Load for Diagnosis CSV\n",
    "@dlt.table(\n",
    "    comment=\"Bronze Load for Diagnosis CSV\",\n",
    "    table_properties={\n",
    "        \"quality\": \"bronze\",\n",
    "        \"pipelines.autoOptimize.managed\": \"true\"\n",
    "    }\n",
    ")\n",
    "def BronzeDiagnosis():\n",
    "    df = spark.read.format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .load(\"/Volumes/capstone_hospital/data/raw/diagnosis_ref.csv\")\n",
    "    return df\n",
    "\n",
    "\n",
    "#Bronze Load for Claim CSV\n",
    "@dlt.table(\n",
    "    comment=\"Bronze Load for Claim CSV\",\n",
    "    table_properties={\n",
    "        \"quality\": \"bronze\",\n",
    "        \"pipelines.autoOptimize.managed\": \"true\"\n",
    "    }\n",
    ")\n",
    "def BronzeClaim():\n",
    "    df = spark.read.format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .load(\"/Volumes/capstone_hospital/data/raw/claims_batch.csv\")\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "217f9d71-2c25-4afd-b1ad-d0d3cd329fc6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ingest claims & provider json stream into bronze"
    }
   },
   "outputs": [],
   "source": [
    "# Bronze Load for Claims Stream JSON\n",
    "@dlt.table(\n",
    "    comment=\"Bronze Load for Claims Stream JSON\",\n",
    "    table_properties={\n",
    "        \"quality\": \"bronze\",\n",
    "        \"pipelines.autoOptimize.managed\": \"true\"\n",
    "    }\n",
    ")\n",
    "def BronzeClaimsStream():\n",
    "    return (\n",
    "        spark.read.format(\"json\")\n",
    "        .load(\"/Volumes/capstone_hospital/data/raw/claims_stream.json\"))\n",
    "\n",
    "# Bronze Load for Providers JSON\n",
    "@dlt.table(\n",
    "    comment=\"Bronze Load for Providers JSON\",\n",
    "    table_properties={\n",
    "        \"quality\": \"bronze\",\n",
    "        \"pipelines.autoOptimize.managed\": \"true\"\n",
    "    }\n",
    ")\n",
    "def BronzeProviders():\n",
    "    return (\n",
    "        spark.read.format(\"json\")\n",
    "        .load(\"/Volumes/capstone_hospital/data/raw/providers.json\")\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3a50694-de1d-4b7c-8f53-387fbfb68971",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "tranform into siver"
    }
   },
   "outputs": [],
   "source": [
    "# SilverView: Member : cleaned data including quality checks\n",
    "@dlt.table(\n",
    "  name=\"SilverMembers\",\n",
    "  comment=\"Cleaned and deduplicated for Silver sales data\",\n",
    "   table_properties={\n",
    "        \"quality\": \"silver\",\n",
    "        \"pipelines.autoOptimize.managed\": \"true\"\n",
    "    }\n",
    ")\n",
    "def SilverMembers():\n",
    "    df = dlt.read(\"BronzeMembers\")\n",
    "    return df.filter(col(\"memberID\").isNotNull()).dropDuplicates([\"memberID\"])\n",
    "\n",
    "# SilverView: diagnosis : cleaned data including quality checks\n",
    "@dlt.table(\n",
    "    name=\"SilverDiagnosis\",\n",
    "    comment=\"Cleaned data including quality checks\",\n",
    "    table_properties={\n",
    "        \"quality\": \"silver\",\n",
    "        \"pipelines.autoOptimize.managed\": \"true\"\n",
    "    }\n",
    ")\n",
    "def SilverDiagnosis():\n",
    "    return dlt.read(\"BronzeDiagnosis\").filter(col(\"code\").isNotNull()).dropDuplicates([\"code\"])\n",
    "\n",
    "# SilverView: diagnosis : cleaned data including quality checks\n",
    "@dlt.table(\n",
    "    name=\"SilverClaim\",\n",
    "    comment=\"Cleaned data including quality checks\",\n",
    "    table_properties={\n",
    "        \"quality\": \"silver\",\n",
    "        \"pipelines.autoOptimize.managed\": \"true\"\n",
    "    }\n",
    ")\n",
    "def SilverClaim():\n",
    "    return dlt.read(\"BronzeClaim\").filter(col(\"ClaimID\").isNotNull()).dropDuplicates([\"ClaimID\"])    \n",
    "\n",
    " # SilverView: claims stream : cleaned data including quality checks\n",
    "@dlt.table(\n",
    "    name=\"silveClaimsStream\",\n",
    "    comment=\"Cleaned data including quality checks\",\n",
    "    table_properties={\n",
    "        \"quality\": \"silver\",\n",
    "        \"pipelines.autoOptimize.managed\": \"true\"\n",
    "    }\n",
    ")\n",
    "def SilverClaim():\n",
    "    return dlt.read(\"BronzeClaimsStream\").filter(col(\"ClaimID\").isNotNull()).dropDuplicates([\"ClaimID\"])    \n",
    "\n",
    " # SilverView: provider : cleaned data including quality checks\n",
    "@dlt.table(\n",
    "    name=\"SilverProviders\",\n",
    "    comment=\"Cleaned data including quality checks\",\n",
    "    table_properties={\n",
    "        \"quality\": \"silver\",\n",
    "        \"pipelines.autoOptimize.managed\": \"true\"\n",
    "    }\n",
    ")\n",
    "def SilverClaim():\n",
    "    return dlt.read(\"BronzeProviders\").filter(col(\"ProviderID\").isNotNull()).dropDuplicates([\"ProviderID\"])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7767e1a-4d94-4b3c-b9b7-12fc8b188eb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name=\"GoldFraudDetection\",\n",
    "    comment=\"Joined with Filtered data for Fraud Detection\"\n",
    ")\n",
    "def GoldFraudDetection():\n",
    "    # Load the tables\n",
    "    SilverClaim = spark.read.table(\"capstone_hospital.default.SilverClaim\")\n",
    "    SilverDiagnosis = spark.read.table(\"capstone_hospital.default.SilverDiagnosis\")\n",
    "    SilverMembers = spark.read.table(\"capstone_hospital.default.SilverMembers\")\n",
    "\n",
    "    # Explode ICD10Codes into multiple rows\n",
    "    exploded_claims = SilverClaim.withColumn(\"ICD10Code\", explode(split(col(\"ICD10Codes\"), \",\")))\n",
    "\n",
    "    # Filter rows where ClaimDate is more than 24 hours before ServiceDate\n",
    "    filtered_claims = exploded_claims.filter(datediff(col(\"ClaimDate\"), col(\"ServiceDate\")) > 1)\n",
    "\n",
    "    # Join the tables\n",
    "    result = filtered_claims.join(SilverMembers, \"MemberID\") \\\n",
    "                            .join(silver_diagnosis, filtered_claims[\"ICD10Code\"] == silver_diagnosis[\"code\"])\n",
    "\n",
    "    return result"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ingest Bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
